# Kafka security configuration file
java.security.auth.login.config=/example_path/cfe_39/etc/config.jaas
# Logger settings
log4j2.configurationFile=/example_path/cfe_39/etc/log4j2.properties
# What topics are searched from kafka, regex
queueTopicPattern=^testConsumerTopic-*$
# Number of consumers created to the consumer groups
numOfConsumers=2
# Kafka bootstrap servers - 127.0.0.1:9094,127.0.0.2:9094,127.0.0.3:9094
consumer.bootstrap.servers=test
# Offset, should not be touched
consumer.auto.offset.reset=earliest
# Autocommit, should not be touched
consumer.enable.auto.commit=false
# Consumer group id, this is to track the progress of reading hte topic
consumer.group.id=cfe_39
# Used security protocol and mechanism
consumer.security.protocol=SASL_PLAINTEXT
consumer.sasl.mechanism=PLAIN
# Maximum records per batch, note that too big number will cause massive load and can cause timeouts to trigger
consumer.max.poll.records=500
# How much data can be fetched in one go
consumer.fetch.max.bytes=1073741820
# How long for request before timing out. Note that too big max poll records size can cause this to trigger
consumer.request.timeout.ms=300000
consumer.max.poll.interval.ms=300000
# For testing only, remove for prod.
consumer.useMockKafkaConsumer=true
# AVRO
queueDirectory=/example_path/cfe_39/etc/AVRO/
queueNamePrefix=testingAVRO
# The maximum file size for AVRO-files that are to be stored in HDFS database.
maximumFileSize=3000
# HDFS pruning, use 157784760000 value while testing HDFS writes to ensure the test records are not pruned.
pruneOffset=157784760000